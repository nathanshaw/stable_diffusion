{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ebb2094-c4f2-4f8b-83b3-0f89da355b6c",
   "metadata": {},
   "source": [
    "In this guide, we will show how to generate novel images based on a text prompt using the KerasCV implementation of stability.ai's text-to-image model, Stable Diffusion.\n",
    "\n",
    "Stable Diffusion is a powerful, open-source text-to-image generation model. While there exist multiple open-source implementations that allow you to easily create images from textual prompts, KerasCV's offers a few distinct advantages. These include XLA compilation and mixed precision support, which together achieve state-of-the-art generation speed.\n",
    "\n",
    "In this guide, we will explore KerasCV's Stable Diffusion implementation, show how to use these powerful performance boosts, and explore the performance benefits that they offer.\n",
    "\n",
    "To get started, let's install a few dependencies and sort out some imports:\n",
    "pip3 install tensorflow keras_cv --upgrade --quiet\n",
    "pip3 install PILLOW keras_cv pycocotools pyococotools\n",
    "apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a37b4b-e28c-4eae-ae2f-c499dfc05b1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 15:58:26.685930: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You do not have Waymo Open Dataset installed, so KerasCV Waymo metrics are not available.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import keras_cv\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "110252c3-7a18-4f11-a3bc-a5948155e370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nathan/workspace/stable_diffusion/output_images/2023_04_13_15_\n",
      "By using this model checkpoint, you acknowledge that its usage is subject to the terms of the CreativeML Open RAIL-M license at https://raw.githubusercontent.com/CompVis/stable-diffusion/main/LICENSE\n"
     ]
    }
   ],
   "source": [
    "# create a basic naming root for any files that we create and want to save during this session\n",
    "time_str = datetime.now().strftime(\"%Y_%m_%d_%H_\")\n",
    "output_name = os.getcwd() + \"/output_images/\" + time_str\n",
    "img_num = 0\n",
    "print(output_name)\n",
    "# model = keras_cv.models.StableDiffusion(img_width=512, img_height=512)\n",
    "\n",
    "# MQ model for standard operation speed\n",
    "# model = keras_cv.models.StableDiffusion(img_width=512, img_height=512, jit_compile=True)\n",
    "# LQ and fast version of the model\n",
    "model = keras_cv.models.StableDiffusion(img_width=96, img_height=96, jit_compile=False)\n",
    "# HQ and slow version\n",
    "#model = keras_cv.models.StableDiffusion(img_width=512, img_height=512, jit_compile=True)\n",
    "\n",
    "# simple function to load image\n",
    "def load_img(path_to_img):\n",
    "  max_dim = 512\n",
    "  img = tf.io.read_file(path_to_img)\n",
    "  img = tf.image.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "  long_dim = max(shape)\n",
    "  scale = max_dim / long_dim\n",
    "\n",
    "  new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "  img = tf.image.resize(img, new_shape)\n",
    "  img = img[tf.newaxis, :]\n",
    "  return img\n",
    "\n",
    "# simple function to display image\n",
    "def imshow(image, title=None):\n",
    "  if len(image.shape) > 3:\n",
    "    image = tf.squeeze(image, axis=0)\n",
    "\n",
    "  plt.imshow(image)\n",
    "  if title:\n",
    "    plt.title(title)\n",
    "\n",
    "def save_images(images):\n",
    "    global img_num\n",
    "    for i in range(len(images)):\n",
    "        tf.keras.utils.save_img(output_name + str(img_num) + \".png\", images[i])\n",
    "        img_num = img_num + 1\n",
    "\n",
    "def save_image(image):\n",
    "    global img_num\n",
    "    tf.keras.utils.save_img(output_name + str(img_num) + \".png\", np.squeeze(image, 0))\n",
    "    img_num = img_num + 1\n",
    "\n",
    "def plot_images(images):\n",
    "    print(\"images length: \", len(images))\n",
    "    if len(images) > 1:\n",
    "        nrows, hop_size = adjustGrid(len(images))\n",
    "        plt.figure(figsize=(hop_size*10, nrows*10))\n",
    "        plt.margins(0.0)\n",
    "        # for each lines of the plo%\n",
    "        print(\"plotting {} total images with {} columns in {} rows\".format(len(images), hop_size, nrows))\n",
    "        for i in range(len(images)):\n",
    "            ax = plt.subplot(nrows, hop_size, i+1)\n",
    "            plt.imshow(images[i])\n",
    "            plt.axis(\"off\")\n",
    "            plt.savefig(output_name + str(img_num) + \"_figure.png\")\n",
    "    else:\n",
    "      plot_image(image)\n",
    "\n",
    "def plot_image(image):\n",
    "    image = np.squeeze(image, 0)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.margins(0.0)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(output_name + str(img_num) + \"_figure.png\")\n",
    "\n",
    "def singleImage(source, style):\n",
    "    output_images = []\n",
    "    source_image = load_img(source)\n",
    "    style_image = load_img(style)\n",
    "    output_images.extend(hub_model(tf.constant(source_image), tf.constant(style_image))[0])\n",
    "    save_images(output_images)\n",
    "    output_images.extend(style_image)\n",
    "    output_images.extend(source_image)\n",
    "    output_images.reverse()\n",
    "    plot_images(output_images)\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "  tensor = tensor*255\n",
    "  tensor = np.array(tensor, dtype=np.uint8)\n",
    "  if np.ndim(tensor)>3:\n",
    "    assert tensor.shape[0] == 1\n",
    "    tensor = tensor[0]\n",
    "  return PIL.Image.fromarray(tensor)\n",
    "\n",
    "\n",
    "def adjustGrid(num, cmax=5):\n",
    "    # find place where we \"flip\", use that to determine number of rows and columns\n",
    "    # go from 1 to half of number\n",
    "    rnum = 1\n",
    "    cnum = num\n",
    "    for rn in range(1, ceil(num/2) +1):\n",
    "        rnum = rn\n",
    "        cnum = ceil(num/rnum)\n",
    "        # if the number of columns is no longer greater than the number of rows, undo the change and export numbers\n",
    "        if cnum <= rnum and cnum <= cmax:\n",
    "            break\n",
    "    total = rnum * cnum\n",
    "    # if total < num:\n",
    "    # rnum = rnum + 1\n",
    "    return (rnum, cnum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "972aa552",
   "metadata": {},
   "source": [
    "THIS BELOW CELL IS FOR CREATING A SINGLE OUTPUT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14d79e1b-77a1-4227-938f-89d68e886287",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 15:58:35.081296: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "16/16 [==============================] - 115s 5s/step\n",
      "images length:  8\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ceil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m      7\u001b[0m images \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtext_to_image(\n\u001b[1;32m      8\u001b[0m     prompt,\n\u001b[1;32m      9\u001b[0m     seed\u001b[39m=\u001b[39mseed,\n\u001b[1;32m     10\u001b[0m     num_steps\u001b[39m=\u001b[39msteps,\n\u001b[1;32m     11\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[39m# keras.backend.clear_session()  # Clear session to preserve memory\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m plot_images(images)\n\u001b[1;32m     16\u001b[0m img_num \u001b[39m=\u001b[39m save_images(images, img_num)\n",
      "Cell \u001b[0;32mIn[2], line 55\u001b[0m, in \u001b[0;36mplot_images\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mimages length: \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(images))\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(images) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 55\u001b[0m     nrows, hop_size \u001b[39m=\u001b[39m adjustGrid(\u001b[39mlen\u001b[39;49m(images))\n\u001b[1;32m     56\u001b[0m     plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(hop_size\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m, nrows\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m))\n\u001b[1;32m     57\u001b[0m     plt\u001b[39m.\u001b[39mmargins(\u001b[39m0.0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 101\u001b[0m, in \u001b[0;36madjustGrid\u001b[0;34m(num, cmax)\u001b[0m\n\u001b[1;32m     99\u001b[0m rnum \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    100\u001b[0m cnum \u001b[39m=\u001b[39m num\n\u001b[0;32m--> 101\u001b[0m \u001b[39mfor\u001b[39;00m rn \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, ceil(num\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    102\u001b[0m     rnum \u001b[39m=\u001b[39m rn\n\u001b[1;32m    103\u001b[0m     cnum \u001b[39m=\u001b[39m ceil(num\u001b[39m/\u001b[39mrnum)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ceil' is not defined"
     ]
    }
   ],
   "source": [
    "batch = 3\n",
    "seed = 10\n",
    "steps = 40\n",
    "prompt =  \"balls to the walls, highly detailed, fantasy, character art, strong woman, extreme, action, photograph\"\n",
    "\n",
    "#########################################################\n",
    "images = model.text_to_image(\n",
    "    prompt,\n",
    "    seed=seed,\n",
    "    num_steps=steps,\n",
    "    batch_size=batch,\n",
    ")\n",
    "# keras.backend.clear_session()  # Clear session to preserve memory\n",
    "\n",
    "plot_images(images)\n",
    "img_num = save_images(images, img_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This pannel is for easily producing several images using different prompts\n",
    "batch = 1\n",
    "seed = 2\n",
    "steps = 5\n",
    "prompts =  [\n",
    "    \"balls to the walls, cubist, sports, inspirational, walls, balls, fantasy, photograph\", \n",
    "    \"cute and adorable elephants, sureal, salvidore dali, melting\"\n",
    "]\n",
    "\n",
    "def convertMultiple(prompts, batch=1, seed=None, steps=20):\n",
    "    # empty out the images list so I dont accidently save stuff from prior runs of the code\n",
    "    keras.backend.clear_session()  # Clear session to preserve memory\n",
    "    outputs = []\n",
    "    for i in range(len(prompts)):\n",
    "        print(\"running prompt #\",i, \" : \", prompts[i])\n",
    "        t_images = model.text_to_image(\n",
    "            prompts[i],\n",
    "            seed=seed,\n",
    "            num_steps=steps,\n",
    "            batch_size=batch,\n",
    "        )\n",
    "        outputs.extend(t_images)\n",
    "    return outputs\n",
    "\n",
    "images = convertMultiple(prompts, batch=batch, seed=seed, steps=steps)\n",
    "plot_images(images)\n",
    "img_num = save_images(images, img_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ccd435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is for saving each step of the process as it's on image\n",
    "seed = 0\n",
    "steps = 4\n",
    "# how often to output an image\n",
    "mod = 1\n",
    "prompt = \"phychadelic elephants dancing with umbrellas, elephants on stilts\"\n",
    "\n",
    "def save_each_ittr(prompt, seed=None, steps=20, mod=5):\n",
    "    outputs = []\n",
    "    for i in range (1, steps+1, mod):\n",
    "        keras.backend.clear_session() \n",
    "        output = model.text_to_image(\n",
    "            prompt,\n",
    "            seed=seed,\n",
    "            num_steps=i,\n",
    "            batch_size=1,\n",
    "        )\n",
    "        print(\"created image with prompt {} and iter_num {}\".format(prompt, i))\n",
    "        outputs.extend(output)\n",
    "    return outputs\n",
    "\n",
    "images = save_each_ittr(prompt, seed=0, steps=20, mod=5)\n",
    "plot_images(images)\n",
    "img_num = save_images(images, img_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18250fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3655ac51d38ab7348615ff741009be41ced8003e01376576c0c8296826e6db1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
